{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Misspecification in SBI\n",
    "\n",
    "In this tutorial, we would describe how to detect prior misspecification, one instance of model misspecification in Simulation-based Inference (SBI), and demonstrate it on two simulators: a toy 2d Gaussian mean inference task, and the [Hodgkin-Huxley\n",
    "model](https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model) from neuroscience. General familiarity with the SBI toolkit will be assumed.\n",
    "\n",
    "\n",
    "References:\n",
    "- [1] [Schmitt et al, 2024](https://arxiv.org/abs/2112.08866)\n",
    "- [2] [Kelly et al, 2025](https://arxiv.org/abs/2503.12315)\n",
    "- [3] [Gretton et al, 2012](https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "\n",
    "from sbi.diagnostics.misspecification import (\n",
    "    calc_misspecification_mmd,  # Import the MMD diagnostic\n",
    ")\n",
    "from sbi.inference import NPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "\n",
    "# Set seed\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove top and right axis from plots\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.spines.top\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prior Misspecification Example on the Gaussian Simulator\n",
    "\n",
    "In this example, we work with the following simulator: the prior $\\theta \\sim \\mathcal{N}(0, I_d)$, and the observations $x|\\theta \\sim \\mathcal{N}(\\theta, I_d)$ where $d$ is the dimensionality of the problem. For the purposes of this example, we would assume the observations, i.e., $x_o$ would come from the (true) data generating process above.\n",
    "\n",
    "Now we demonstrate a concrete example of model misspecification scenario in SBI. We assume our posterior inference network (e.g., NPE) would be fitted on $(\\theta, x)$ pairs where the $\\theta$ values are sampled from $\\mathcal{N}(\\mu, I_d)$ (instead of $\\mathcal{N}(0, I_d)$, i.e., with some offset mean $\\mu$), and the corresponding observations $x$ were generated using the simulator as above. We refer to this manifestation of model misspecification as **prior misspecification**, and demonstrate how we can identify this using (one or many) observations $x_o$ during test time, following the maximum mean discrepancy (MMD) based approach outlined in [1]. \n",
    "\n",
    "The core idea of the method is to compute a distribution of MMD values among the synthetic simulations ($x$) the model was trained on, and perform an out-of-distribution/p-value check for the MMD between the synthetic simulations and the $x_o$. Since we will often encounter only one observation $x_o$ during inference, we would make use of the biased version of sample-based MMD recommended in [3]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ground-truth and misspecified priors\n",
    "The \"ground-truth\" prior simply means the $x_{obs}$ will come from this prior distribution through the simulator.\n",
    "The misspecified prior: the NPE will be trained on $(\\theta, x)$ pairs coming from this prior. Note that the simulator code is same in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2  # observation dimension\n",
    "# true prior -- the observation comes from here\n",
    "mean_true = torch.zeros(dim)\n",
    "cov_true = torch.eye(dim)\n",
    "prior_true = dist.MultivariateNormal(loc=mean_true, covariance_matrix=cov_true)\n",
    "\n",
    "\n",
    "# the NPE will be trained on samples from this misspecified prior\n",
    "def give_misspec_prior(mu0, tau0):\n",
    "    if mu0.ndim > 1:\n",
    "        raise ValueError(\"mu0 should be a 1d tensor of shape [dim]\")\n",
    "    dim = mu0.shape[0]\n",
    "    return dist.MultivariateNormal(loc=mu0, covariance_matrix=tau0 * torch.eye(dim))\n",
    "\n",
    "\n",
    "offset = 4\n",
    "mu0 = mean_true + offset  # just offset, applies in all directions\n",
    "tau0 = 1.0  # 1.0 means no change in covariance matrix\n",
    "prior_mis = give_misspec_prior(mu0, tau0)\n",
    "\n",
    "\n",
    "def simulator(theta):\n",
    "    return theta + torch.randn_like(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training dataset for the NPE, i.e., $(\\theta, x)$ pairs\n",
    "As we will see later, we can only train one NPE on the well-specified dataset, and simulate the other scenarios exploiting the symmetry present in our setting.\n",
    "We will also generate a validation dataset, to compute many _self_ MMDs or the distribution of MMDs which will be utilised later for the actual misspecification check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 1000\n",
    "\n",
    "# generate training data for clean/well-specified model\n",
    "theta_well = prior_true.sample((num_simulations,))\n",
    "x_well = simulator(theta_well)\n",
    "\n",
    "\n",
    "# validation set to compute MMD distribution in the well-specified case\n",
    "# this could just be a subset of the training data\n",
    "\n",
    "num_validations_mmd = 1000\n",
    "theta_val_well = prior_true.sample((num_validations_mmd,))\n",
    "x_val_well = simulator(theta_val_well)\n",
    "\n",
    "print(theta_well.shape, theta_val_well.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our inference object\n",
    "We are only training one NPE inference with an embedding network to demonstrate the MMD-based misspecification check both on the original _x-space_ and the _embedding_ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_npe_with_embedding(theta, x, prior, embeddding_net, **kwargs):\n",
    "    neural_posterior = posterior_nn(model=\"maf\", embedding_net=embeddding_net)\n",
    "    inference = NPE(prior=prior, density_estimator=neural_posterior, **kwargs)\n",
    "    inference = inference.append_simulations(theta, x)\n",
    "    _ = inference.train()\n",
    "    return inference\n",
    "\n",
    "\n",
    "emb_net_well = FCEmbedding(\n",
    "    input_dim=dim, output_dim=dim, num_layers=2, num_hiddens=20\n",
    ")  # minimal embedding network\n",
    "NPE_well_embd = train_npe_with_embedding(\n",
    "    theta_well, x_well, prior=prior_true, embeddding_net=emb_net_well\n",
    ")  # modified the emb_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the observations $x_{obs}$ to do inference\n",
    "We will generate two observations: one from the ground-truth prior to demonstrate the case where we **do not** have any model misspecification. The second observation will be generated from the misspecified prior to _simulate_ the model misspecification scenario when our inference was trained on the misspecified data, but the observation comes from the ground-truth prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do inference given observed data\n",
    "num_observations = 1\n",
    "theta_o = prior_true.sample((num_observations,))\n",
    "x_o = simulator(theta_o)\n",
    "\n",
    "# print(f\"theta_o (comes from true prior): {theta_o}\")\n",
    "# print(f\"x_o (comes from simulator using theta_o): {x_o}\")\n",
    "\n",
    "# we can also create observation from the misspecified prior x_o_mis\n",
    "theta_o_mis = prior_mis.sample((num_observations,))\n",
    "x_o_mis = simulator(theta_o_mis)\n",
    "\n",
    "# print(f\"theta_o_mis (comes from misspecified prior): {theta_o_mis}\")\n",
    "# print(f\"x_o_mis (comes from simulator using theta_o_mis): {x_o_mis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_val_well[:, 0], x_val_well[:, 1], \"o\", label=\"well-specified x\")\n",
    "plt.plot(x_o_mis[:, 0], x_o_mis[:, 1], \"o\", color=\"red\", label=r\"misspecified $x_o$\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misspecification detection\n",
    "Here we only demonstrate misspecification detection on the embedding space. Note that the corresponding scenarios on the x-space could be simply tested by passing `inference=None` and `mode=x_space` in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val, (mmds_baseline, mmd) = calc_misspecification_mmd(\n",
    "    inference=NPE_well_embd, x_obs=x_o, x=x_val_well, mode='embedding'\n",
    ")\n",
    "\n",
    "print(f\"p-val: {p_val:.6f}\")\n",
    "plt.hist(mmds_baseline.numpy(), bins=50, label='baseline')\n",
    "plt.axvline(mmd.item(), color=\"red\", label=r'MMD(x, $x_o$)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('MMD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** In the above example, the observed data $x_o$ comes from the same ground truth prior as what the NPE estimator was trained on. As detected by the check above, the $p$-value for the null hypothesis $H_0$ (that the distribution of MMDs between samples in `x_val_well` (intra) and the distribution of MMDs between `x_val_well` and `x_o` are same) came out to be $0.501 \\gt 0.05$, so we fail to reject the null hypothesis, i.e., no misspecification is detected in this case.\n",
    "\n",
    "Now we turn to the situation where there is a misspecification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val, (mmds_baseline, mmd) = calc_misspecification_mmd(\n",
    "    inference=NPE_well_embd, x_obs=x_o_mis, x=x_val_well, mode='embedding'\n",
    ")\n",
    "\n",
    "print(f\"p-val: {p_val}\")\n",
    "plt.hist(mmds_baseline.numpy(), bins=50, label='baseline')\n",
    "plt.axvline(mmd.item(), color=\"red\", label=r'MMD(x, $x_o$)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('MMD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, in this case the $p$-value is $0.0$ so we reject the null hypothesis, and can warn the user that there might be mismatch in distribution between the observed sample $x_o$, and the dataset on which the inference network was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misspecification on Hodgkin-Huxley model: tutorial\n",
    "\n",
    "In this tutorial, we will check model misspecification on a [Hodgkin-Huxley\n",
    "model](https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model) from\n",
    "neuroscience (Hodgkin and Huxley, 1952). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you find a tutorial on the HH model in the `sbi` repository under\n",
    "[docs/tutorials/Example_00_HodgkinHuxleyModel.ipynb](https://github.com/sbi-dev/sbi/blob/main/docs/tutorials/Example_00_HodgkinHuxleyModel.ipynb).\n",
    "\n",
    "Here we assume, that you are already familiar with the Hodgkin-Huxley model and the basic functionality of `sbi`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to import basic packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# HH simulator\n",
    "from HH_helper_functions import HHsimulator, calculate_summary_statistics, syn_current\n",
    "\n",
    "from sbi import analysis as analysis\n",
    "\n",
    "# sbi\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import simulate_for_sbi\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove top and right axis from plots\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.spines.top\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Observed data\n",
    "\n",
    "Let us assume we current-clamped a neuron and recorded the following voltage trace:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mackelab/delfi/master/docs/docs/tutorials/observed_voltage_trace.png\" width=\"480\">\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulator\n",
    "\n",
    "We would like to infer the posterior over the two parameters ($\\color{orange}{\\bar g_{Na}}$,$\\color{orange}{\\bar g_K}$) of a Hodgkin-Huxley model, given the observed electrophysiological recording above. The model has channel kinetics as in [Pospischil et al. 2008](https://link.springer.com/article/10.1007/s00422-008-0263-8), and is defined by the following set of differential equations (parameters of interest highlighted in orange):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\scriptsize\n",
    "\\begin{align}\n",
    "\\color{black}{C_m\\frac{dV}{dt}}& \\color{black}{=g_1\\left(E_1-V\\right)}+\n",
    "                    \\color{orange}{\\bar{g}_{Na}} \\color{black}{m^3h\\left(E_{Na}-V\\right)+}\n",
    "                    \\color{orange}{\\bar{g}_{K}} \\color{black}{n^4\\left(E_K-V\\right)+\\bar{g}_Mp\\left(E_K-V\\right)+I_{inj}+\\sigma\\eta\\left(t\\right)}\\\\\n",
    "                    \\color{black}{\\frac{dq}{dt}}&\\color{black}{=\\frac{q_\\infty\\left(V\\right)-q}{\\tau_q\\left(V\\right)},\\;q\\in\\{m,h,n,p\\}}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current, onset time of stimulation, offset time of stimulation, time step, time, area of some\n",
    "I_inj, t_on, t_off, dt, t, A_soma = syn_current()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_HH_model(params):\n",
    "\n",
    "    params = np.asarray(params)\n",
    "\n",
    "    # input current, time step\n",
    "    I_inj, t_on, t_off, dt, t, A_soma = syn_current()\n",
    "\n",
    "    t = np.arange(0, len(I_inj), 1) * dt\n",
    "\n",
    "    # initial voltage V0\n",
    "    initial_voltage = -70\n",
    "\n",
    "    voltage_trace = HHsimulator(initial_voltage, params.reshape(1, -1), dt, t, I_inj)\n",
    "\n",
    "    return dict(data=voltage_trace.reshape(-1), time=t, dt=dt, I_inj=I_inj.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for convenience we define the simulator to return only the voltage trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(params):\n",
    "    \"\"\"\n",
    "    Returns only voltage trace\n",
    "    \"\"\"\n",
    "    obs = run_HH_model(params)\n",
    "    return torch.tensor(obs[\"data\"], dtype=torch.float).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the output of the Hodgkin-Huxley model, let us generate some voltage traces for different parameters ($\\bar g_{Na}$,$\\bar g_K$), given the input current $I_{\\text{inj}}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three sets of (g_Na, g_K)\n",
    "params = np.array([[10.0, 5.0], [4.0, 1.5], [20.0, 10.0]])\n",
    "\n",
    "num_samples = len(params[:, 0])\n",
    "sim_samples = np.zeros((num_samples, len(I_inj)))\n",
    "for i in range(num_samples):\n",
    "    sim_samples[i, :] = run_HH_model(params=params[i, :])[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for traces\n",
    "col_min = 2\n",
    "num_colors = num_samples + col_min\n",
    "cm1 = mpl.cm.Blues\n",
    "col1 = [cm1(1.0 * i / num_colors) for i in range(col_min, num_colors)]\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "gs = mpl.gridspec.GridSpec(2, 1, height_ratios=[4, 1])\n",
    "ax = plt.subplot(gs[0])\n",
    "# plot the three voltage traces for different parameter sets\n",
    "for i in range(num_samples):\n",
    "    plt.plot(t, sim_samples[i, :], color=col1[i], lw=2, label=i)\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"voltage (mV)\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([-80, -20, 40])\n",
    "\n",
    "# plot the injected current\n",
    "ax = plt.subplot(gs[1])\n",
    "plt.plot(t, I_inj * A_soma * 1e3, \"k\", lw=2)\n",
    "plt.xlabel(\"time (ms)\")\n",
    "plt.ylabel(\"input (nA)\")\n",
    "\n",
    "ax.set_xticks([0, max(t) / 2, max(t)])\n",
    "ax.set_yticks([0, 1.1 * np.max(I_inj * A_soma * 1e3)])\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(\"%.2f\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prior over model parameters\n",
    "\n",
    "Now that we have the simulator, we need to define a function with the prior over the model parameters ($\\bar g_{Na}$,$\\bar g_K$), which in this case is chosen to be a Uniform distribution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: This is where you would incorporate prior knowlegde about the parameters you want to infer, e.g., ranges known from literature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well specified prior:\n",
    "# prior_min = [0.5, 1e-4] # g_Na, g_K\n",
    "# prior_max = [80.0, 15.0]\n",
    "\n",
    "# misspecified prior:\n",
    "prior_min = [0.5, 1e-4]\n",
    "prior_max = [40.0, 5]\n",
    "prior = utils.torchutils.BoxUniform(\n",
    "    low=torch.as_tensor(prior_min), high=torch.as_tensor(prior_max)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "\n",
    "theta_train, x_train = simulate_for_sbi(\n",
    "    simulator, proposal=prior, num_simulations=500, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate misspecified sample\n",
    "params_mis = np.array([70, 15])\n",
    "x_o_mis = torch.tensor(\n",
    "    run_HH_model(params=params_mis)[\"data\"], dtype=torch.float\n",
    ").reshape(1, -1)\n",
    "\n",
    "# and well specified\n",
    "params_o = np.array([10, 4])\n",
    "x_o = torch.tensor(run_HH_model(params=params_o)[\"data\"], dtype=torch.float).reshape(\n",
    "    1, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, x_train[:20, :].T, alpha=0.5)\n",
    "plt.ylabel(\"voltage (mV)\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([-80, -20, 40])\n",
    "plt.plot(t, x_o_mis[0], color=\"red\", label=\"misspecified data\")\n",
    "plt.plot(t, x_o[0], color=\"black\", label=\"well-specified data\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Misspecification on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.diagnostics.misspecification import calc_misspecification_mmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the well specified observation `x_o`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val, (mmds_baseline, mmd) = calc_misspecification_mmd(\n",
    "    inference=None,\n",
    "    x_obs=x_o,\n",
    "    x=x_train,\n",
    "    n_shuffle=1_000,\n",
    "    max_samples=1_000,\n",
    ")\n",
    "\n",
    "print(f\"p-value: {p_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the misspecified observation `x_o_mis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_mis, (_, mmd_mis) = calc_misspecification_mmd(\n",
    "    inference=None,\n",
    "    x_obs=x_o_mis,\n",
    "    x=x_train,\n",
    "    n_shuffle=1_000,\n",
    "    max_samples=1_000,\n",
    ")\n",
    "\n",
    "print(f\"p-value: {p_val_mis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the observed value `x_o_mis` is very unlikely under the null hypothesis, which states that that the observation is coming from the true data distribution $p(x)$.\n",
    "\n",
    "We can therefore reject $H_0$ and have evidence that `x_o_mis` is coming from a different distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Misspecification based on summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as many neuroscientists work on summary statistics for the Hodgking Huxley model, let's do the same analysis on the preprocessed data.\n",
    "\n",
    "Let's first define an augmented simulator which returns the summary statistics directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator_sumstats(params):\n",
    "    \"\"\"\n",
    "    Returns summary statistics from conductance values in `params`.\n",
    "\n",
    "    Summarizes the output of the HH simulator and converts it to `torch.Tensor`.\n",
    "    \"\"\"\n",
    "    obs = run_HH_model(params)\n",
    "    summstats = torch.as_tensor(calculate_summary_statistics(obs))\n",
    "    return summstats\n",
    "\n",
    "\n",
    "# Check prior, simulator, consistency\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "simulator_sumstats = process_simulator(simulator_sumstats, prior, prior_returns_numpy)\n",
    "check_sbi_inputs(simulator_sumstats, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "theta_train_sumstats, x_train_sumstats = simulate_for_sbi(\n",
    "    simulator_sumstats, proposal=prior, num_simulations=500, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate misspecified sample\n",
    "params_mis = np.array([70, 15])\n",
    "x_raw = run_HH_model(params=params_mis)\n",
    "x_o_mis_sumstats = torch.as_tensor(\n",
    "    calculate_summary_statistics(x_raw), dtype=torch.float\n",
    ").reshape(1, -1)\n",
    "\n",
    "\n",
    "# and well specified\n",
    "params_o = np.array([10, 4])\n",
    "x_raw = run_HH_model(params=params_o)\n",
    "x_o_sumstats = torch.as_tensor(\n",
    "    calculate_summary_statistics(x_raw), dtype=torch.float\n",
    ").reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again first look at the well specified observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val, (mmds_baseline, mmd) = calc_misspecification_mmd(\n",
    "    inference=None,\n",
    "    x_obs=x_o_sumstats,\n",
    "    x=x_train_sumstats,\n",
    "    n_shuffle=1_000,\n",
    "    max_samples=1_000,\n",
    ")\n",
    "\n",
    "print(f\"p-value: {p_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the misspecified observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_mis, (mmds_baseline, mmd) = calc_misspecification_mmd(\n",
    "    inference=None,\n",
    "    x_obs=x_o_mis_sumstats,\n",
    "    x=x_train_sumstats,\n",
    "    n_shuffle=1_000,\n",
    "    max_samples=1_000,\n",
    ")\n",
    "\n",
    "print(f\"p-value: {p_val_mis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we can not reject $H_0$ in this case, although visually the traces look quite different. \n",
    "This is potentially due to the choice of summary statistics together with the fact that we only have tested one single observation `x_o_mis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate misspecified sample\n",
    "params_mis = np.array([[70, 15], [78, 10]])\n",
    "x_raw = []\n",
    "x_o_mis_sumstats = []\n",
    "for i in range(len(params_mis)):\n",
    "    x_raw.append(run_HH_model(params=params_mis[i]))\n",
    "    x_o_mis_sumstats.append(calculate_summary_statistics(x_raw[i]))\n",
    "\n",
    "x_o_mis_sumstats = torch.as_tensor(\n",
    "    np.array(x_o_mis_sumstats), dtype=torch.float\n",
    ").reshape(-1, 7)\n",
    "\n",
    "plt.plot(t, x_train[:20, :].T, alpha=0.5)\n",
    "plt.ylabel(\"voltage (mV)\")\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([-80, -20, 40])\n",
    "for i in range(len(params_mis)):\n",
    "    plt.plot(t, np.array(x_raw[i][\"data\"]), color=\"red\", label=\"misspecified data\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_mis, (mmds_baseline, mmd) = calc_misspecification_mmd(\n",
    "    inference=None,\n",
    "    x_obs=x_o_mis_sumstats,\n",
    "    x=x_train_sumstats,\n",
    "    n_shuffle=10_000,\n",
    ")\n",
    "\n",
    "print(f\"p-value: {p_val_mis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Misspecification in the embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using handcrafted summary statistics we can also use an embedding net $e$ and investigate if the embedded data $z=e(x)$ is misspecified.\n",
    "For this we first have to run inference, to train the embedding net. \n",
    "In this toy example we will use a fully connected net to reduce the dimensionality of the voltage traces. \n",
    "\n",
    "We will use the same training data as in a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train NPE networks\n",
    "emb_net = FCEmbedding(\n",
    "    input_dim=x_train.shape[1], output_dim=20, num_layers=4, num_hiddens=50\n",
    ")  # minimal embedding network\n",
    "neural_posterior = posterior_nn(model=\"maf\", embedding_net=emb_net)\n",
    "inference = NPE(prior=prior, density_estimator=neural_posterior)\n",
    "inference = inference.append_simulations(theta_train, x_train)\n",
    "density_estimator = inference.train()\n",
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((10000,), x=x_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = analysis.pairplot(\n",
    "    samples,\n",
    "    limits=[[0.5, 80], [1e-4, 15.0]],\n",
    "    ticks=[[0.5, 80], [1e-4, 15.0]],\n",
    "    figsize=(5, 5),\n",
    "    points=params_o,\n",
    "    points_offdiag={\"markersize\": 6},\n",
    "    points_colors=\"r\",\n",
    "    # labels=labels_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform two tests for misspecification\n",
    "# 1. well specified model\n",
    "p_val_well, _ = calc_misspecification_mmd(\n",
    "    inference=inference,\n",
    "    x_obs=x_o,\n",
    "    x=x_train,\n",
    "    mode=\"embedding\",\n",
    ")\n",
    "print(f\"p-value well specified: {p_val_well}\")\n",
    "\n",
    "# 2. misspecified model\n",
    "p_val_mis, (mmds_baseline, mmd) = calc_misspecification_mmd(\n",
    "    inference=inference,\n",
    "    x_obs=x_o_mis,\n",
    "    x=x_train,\n",
    "    mode=\"embedding\",\n",
    ")\n",
    "print(f\"p-value misspecified: {p_val_mis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mmds_baseline, bins=50, alpha=0.5, label=\"baseline\")\n",
    "plt.axvline(mmd, color=\"red\", label=\"misspecified\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
